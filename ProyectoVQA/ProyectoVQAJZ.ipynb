{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Desarrollo proyecto VQA por Joaquin Zepeda\n",
        "\n",
        "A continuación se presenta la recopilación de los códigos realizados para la tarea, debido a que el entrenamiento podia demorar más de una hora, se realizaron distintos archivos para ejecutarse en paralelo de forma de ser más eficiente, todo esto se recopila en este archivo."
      ],
      "metadata": {
        "id": "ae56y3w89ZAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Desacarga de los datos"
      ],
      "metadata": {
        "id": "yyjegN_m-BXR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOQp8gEc1qtG",
        "outputId": "75ece0c9-2309-4fea-e4cc-30a0cfe7908c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Questions_Train_abs 100%[===================>]   4.39M  2.82MB/s    in 1.6s    \n",
            "Questions_Val_abstr 100%[===================>]   2.13M  1.58MB/s    in 1.4s    \n",
            "scene_img_abstract_ 100%[===================>]   2.71G  8.74MB/s    in 4m 19s  \n",
            "scene_img_abstract_ 100%[===================>]   1.35G  8.31MB/s    in 2m 32s  \n",
            "Annotations_Train_a 100%[===================>]   2.53M  1.63MB/s    in 1.6s    \n",
            "Annotations_Val_abs 100%[===================>]   1.26M   963KB/s    in 1.3s    \n"
          ]
        }
      ],
      "source": [
        "# Download datasete\n",
        "!wget -q -N --show-progress https://s3.amazonaws.com/cvmlp/vqa/abstract_v002/vqa/Questions_Train_abstract_v002.zip\n",
        "!wget -q -N --show-progress https://s3.amazonaws.com/cvmlp/vqa/abstract_v002/vqa/Questions_Val_abstract_v002.zip\n",
        "!wget -q -N --show-progress https://s3.amazonaws.com/cvmlp/vqa/abstract_v002/scene_img/scene_img_abstract_v002_train2015.zip\n",
        "!wget -q -N --show-progress https://s3.amazonaws.com/cvmlp/vqa/abstract_v002/scene_img/scene_img_abstract_v002_val2015.zip\n",
        "!wget -q -N --show-progress https://s3.amazonaws.com/cvmlp/vqa/abstract_v002/vqa/Annotations_Train_abstract_v002.zip\n",
        "!wget -q -N --show-progress https://s3.amazonaws.com/cvmlp/vqa/abstract_v002/vqa/Annotations_Val_abstract_v002.zip\n",
        "\n",
        "!mkdir -p {Questions/train Questions/test Questions/val Images/train Images/test Images/val Annotations/train Annotations/val}\n",
        "\n",
        "!unzip -q -n Questions_Train_abstract_v002.zip -d Questions\n",
        "!unzip -q -n Questions_Val_abstract_v002.zip -d Questions\n",
        "!unzip -q -n scene_img_abstract_v002_train2015.zip -d Images/train\n",
        "!unzip -q -n scene_img_abstract_v002_val2015.zip -d Images/val\n",
        "!unzip -q -n Annotations_Train_abstract_v002.zip -d Annotations\n",
        "!unzip -q -n Annotations_Val_abstract_v002.zip -d Annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xg7YbvkHChNj"
      },
      "outputs": [],
      "source": [
        "# Remove unnecessary zip files.\n",
        "\n",
        "!rm Questions_Train_abstract_v002.zip\n",
        "!rm Questions_Val_abstract_v002.zip\n",
        "\n",
        "!rm scene_img_abstract_v002_train2015.zip\n",
        "!rm scene_img_abstract_v002_val2015.zip\n",
        "\n",
        "!rm Annotations_Train_abstract_v002.zip\n",
        "!rm Annotations_Val_abstract_v002.zip\n",
        "##########################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inicializamos las semillas"
      ],
      "metadata": {
        "id": "euX61LxO-6KJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qow7zHrFpgSX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flag BOW\n",
        "\n",
        "Si BOW es True, se ejecuta todo lo correspondiente a la primera implementación, la cual corresponde a un modulo de lenguaje BOW + un modulo de vision VGG19 + MLP. En caso contrario se ejecuta la segunda implementación la cual cambia el modelo de lenguaje por una LSTM y mantiene lo demás."
      ],
      "metadata": {
        "id": "Cd11gmrH-USK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1N9I1IFRteS"
      },
      "outputs": [],
      "source": [
        "BOW = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtwdwWUMJE8F"
      },
      "source": [
        "# Codificando las palabras\n",
        "\n",
        "Vamos a representar cada palabra con un único número entero que representará su índice.\n",
        "Para eso construiremos un diccionario que mapee palabras a números. La verdad es que son muy similares los vocabularios, hay muy pequeños cambios."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocabulario para el BoW\n",
        "\n",
        "Se separan los vocabularios para las preguntas y para las respuestas."
      ],
      "metadata": {
        "id": "eoFoyoGx_XCp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhOLbIkGHGwO"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from string import punctuation\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import json\n",
        "\n",
        "def make_bow_qst_vocabulary(input_dir,n_words):\n",
        "    SENTENCE_SPLIT_REGEX = re.compile(r'(\\W+)')\n",
        "    questions_dict = dict()\n",
        "    preguntas = []\n",
        "    datasets = os.listdir(input_dir)\n",
        "    for dataset in ['MultipleChoice_abstract_v002_train2015_questions.json','MultipleChoice_abstract_v002_val2015_questions.json']:\n",
        "        with open(input_dir+'/'+dataset) as f:\n",
        "            questions = json.load(f)['questions']\n",
        "        for question in questions:\n",
        "            all_text = question['question'].lower()\n",
        "            all_text = ''.join([c for c in all_text if c not in punctuation])\n",
        "            words = SENTENCE_SPLIT_REGEX.split(all_text)\n",
        "            preguntas.append(all_text)\n",
        "            for word in words:\n",
        "                word=word.strip()                \n",
        "                if word not in questions_dict:\n",
        "                    questions_dict[word] = 1                \n",
        "                if re.search(r\"[^\\w\\s]\", word):\n",
        "                    continue\n",
        "                questions_dict[word] += 1                \n",
        "    questions_dict = sorted( questions_dict, key= questions_dict.get, reverse=True)\n",
        "    top_answers = ['<unk>'] +  questions_dict[:n_words-1] # '-1' is due to '<unk>'\n",
        "    \n",
        "    return top_answers,  questions_dict[:n_words], preguntas\n",
        "\n",
        "def make_bow_ans_vocabulary(input_dir,n_words):\n",
        "    SENTENCE_SPLIT_REGEX = re.compile(r'(\\W+)')\n",
        "    ans_dict = dict()\n",
        "    datasets = os.listdir(input_dir)\n",
        "    answers = []\n",
        "    for dataset in ['abstract_v002_train2015_annotations.json','abstract_v002_val2015_annotations.json']:\n",
        "        with open(input_dir+'/'+dataset) as f:\n",
        "            annotations = json.load(f)['annotations']\n",
        "        for annotation in annotations:\n",
        "            for answer in annotation['answers']:\n",
        "                word = answer['answer'].strip()\n",
        "                if re.search(r\"[^\\w\\s]\", word):\n",
        "                    continue\n",
        "                if word not in ans_dict:\n",
        "                    ans_dict[word] = 1\n",
        "                ans_dict[word] += 1\n",
        "                \n",
        "    ans_dict = sorted( ans_dict, key= ans_dict.get, reverse=True)\n",
        "    top_answers = ans_dict[:n_words-1] # '-1' is due to '<unk>'\n",
        "    \n",
        "    return top_answers,  ans_dict[:n_words]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vocabulario para la LSTM\n",
        "\n",
        "Se separan nuevamente los vocabularios para las preguntas y respuestas."
      ],
      "metadata": {
        "id": "jOGxgWIhACNZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNpAJaO_L8KM"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from string import punctuation\n",
        "import os\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "def make_vocab_questions(input_dir):\n",
        "    \"\"\"Genera un vocabulario y lo guarda en una lista y diccionario\"\"\"\n",
        "    vocab_set = set()\n",
        "    SENTENCE_SPLIT_REGEX = re.compile(r'(\\W+)')\n",
        "    question_length = []\n",
        "    datasets = os.listdir(input_dir)\n",
        "    preguntas = []\n",
        "    print('Se eliminan los signos de puntuación: ',punctuation)\n",
        "    for dataset in ['MultipleChoice_abstract_v002_train2015_questions.json','MultipleChoice_abstract_v002_val2015_questions.json']:\n",
        "        a = 0    \n",
        "        with open(input_dir+'/'+dataset) as f:\n",
        "            questions = json.load(f)['questions']\n",
        "            if a==0:\n",
        "                print(questions[0]['question'])\n",
        "            a+=1\n",
        "        set_question_length = [None]*len(questions)\n",
        "        for iquestion, question in enumerate(questions):\n",
        "            all_text = question['question'].lower()\n",
        "            all_text = ''.join([c for c in all_text if c not in punctuation])\n",
        "\n",
        "            words = SENTENCE_SPLIT_REGEX.split(all_text)\n",
        "            preguntas.append(all_text)\n",
        "            words = [w.strip() for w in words if len(w.strip()) > 0]\n",
        "            vocab_set.update(words)\n",
        "            set_question_length[iquestion] = len(words)\n",
        "        question_length += set_question_length\n",
        "\n",
        "    vocab_list = list(vocab_set)\n",
        "    vocab_list.sort()\n",
        "    # se agregan 2 preguntas especiales para casos desconocidos\n",
        "    vocab_list.insert(0, '<pad>')\n",
        "    vocab_list.insert(1, '<unk>')\n",
        "    \n",
        "    print('Estamos haciendo un vocabulario con todas las palabras de todas las preguntas')\n",
        "    print('El número total de palabras en las preguntas son: %d' % len(vocab_set))\n",
        "    print('El largo máximo de una pregunta es: %d' % np.max(question_length))\n",
        "\n",
        "    return vocab_list, preguntas\n",
        "\n",
        "\n",
        "def make_vocab_answers(input_dir, n_answers):\n",
        "    \"\"\"Vocabulario con el top n_answers, retorna la lista con el top y las respuestas.\"\"\"\n",
        "    answers = defaultdict(lambda: 0)\n",
        "    datasets = os.listdir(input_dir)\n",
        "    for dataset in ['abstract_v002_train2015_annotations.json','abstract_v002_val2015_annotations.json']:\n",
        "        with open(input_dir+'/'+dataset) as f:\n",
        "            annotations = json.load(f)['annotations']\n",
        "        for annotation in annotations:\n",
        "            for answer in annotation['answers']:\n",
        "                word = answer['answer']\n",
        "                if re.search(r\"[^\\w\\s]\", word):\n",
        "                    continue\n",
        "                answers[word] += 1\n",
        "                \n",
        "    answers = sorted(answers, key=answers.get, reverse=True)\n",
        "    assert('<unk>' not in answers)\n",
        "    top_answers = ['<unk>'] + answers[:n_answers-1] # '-1' is due to '<unk>'\n",
        "    \n",
        "    print('Vocabulario para las respuestas')\n",
        "    print('Número total de respuestas: %d' % len(answers))\n",
        "    print('Guarda el %d de respuestas' % n_answers)\n",
        "    return top_answers, answers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def lookUpBOW(top_words, bow_dict,all_words):\n",
        "    \"\"\"Función auxiliar para generar una Look up table\"\"\"\n",
        "    bow_qns = np.zeros((len(all_words), 1024))\n",
        "\n",
        "    for i in range(len(all_words)):\n",
        "        w = all_words[i]\n",
        "        s =  w.split()\n",
        "        for word in s:\n",
        "            if word not in top_words:\n",
        "                idx = 0\n",
        "            else:\n",
        "                idx = top_words.index(word)\n",
        "            # se suma uno por las palabras repetidas\n",
        "            bow_qns[i][idx] += 1\n",
        "    return bow_qns"
      ],
      "metadata": {
        "id": "22MZQO4e_BpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAYTLiUfIvTq"
      },
      "outputs": [],
      "source": [
        "if bow:\n",
        "    # Se generan los vocabularios y las tablas para ver el funcionameinto.\n",
        "    top_questionsBOW,  qst_bow_dict, preguntas = make_bow_qst_vocabulary(\"Questions\",1024)\n",
        "    top_answersBOW,  ans_bow_dict = make_bow_ans_vocabulary(\"Annotations\",1000)\n",
        "\n",
        "    qst_bow_table = lookUpBOW(top_questionsBOW,  qst_bow_dict, preguntas)\n",
        "    ans_bow_table = lookUpBOW(top_answersBOW,  ans_bow_dict, top_answersBOW)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pruebas"
      ],
      "metadata": {
        "id": "lzuxh_1OBBrB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LysidCEnSCJW",
        "outputId": "8bb1c8a5-233d-42b4-8c3f-7b9ece1b1788"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question who looks happier\n",
            "Question Bow [1. 0. 0. ... 0. 0. 0.]\n",
            "Answer [1. 0. 0. ... 0. 0. 0.]\n",
            "Answer Bow [1. 0. 0. ... 0. 0. 0.]\n",
            "Indice answer 0\n"
          ]
        }
      ],
      "source": [
        "print(\"Question\",preguntas[0])\n",
        "print(\"Question Bow\",qst_bow_table[0])\n",
        "print(\"Answer\",top_answersBOW[0])\n",
        "print(\"Answer Bow\",ans_bow_table[0])\n",
        "print(\"Indice answer\",np.argmax(ans_bow_table[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funciones auxiliares para LSTM\n",
        "\n",
        "Se genera un diccionario para acceder a los indices de las palabras y transformar las palabras e indices de forma bidireccional. Cabe destacar que se eliminan los caracteres especiales, solo se consideran las palabras. Algunas de estas funciones se extrajeron del repositorio basic_vqa que se puede encontrar en las referencias del informe."
      ],
      "metadata": {
        "id": "B7gMF9_LBDGI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnFBDtRWobe4"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "SENTENCE_SPLIT_REGEX = re.compile(r'(\\W+)')\n",
        "\n",
        "def tokenize(sentence):\n",
        "    tokens = SENTENCE_SPLIT_REGEX.split(sentence.lower())\n",
        "    tokens = [t.strip() for t in tokens if len(t.strip()) > 0]\n",
        "    return tokens\n",
        "def load_str_list(fname):\n",
        "    with open(fname) as f:\n",
        "        lines = f.readlines()\n",
        "    lines = [l.strip() for l in lines]\n",
        "    return lines\n",
        "\n",
        "\n",
        "class VocabDict:\n",
        "\n",
        "    def __init__(self, word_list):\n",
        "        self.word_list = word_list\n",
        "        self.word2idx_dict = {w:n_w for n_w, w in enumerate(self.word_list)}\n",
        "        self.vocab_size = len(self.word_list)\n",
        "        self.unk2idx = self.word2idx_dict['<unk>'] if '<unk>' in self.word2idx_dict else None\n",
        "\n",
        "    def idx2word(self, n_w):\n",
        "\n",
        "        return self.word_list[n_w]\n",
        "\n",
        "    def word2idx(self, w):\n",
        "        if w in self.word2idx_dict:\n",
        "            return self.word2idx_dict[w]\n",
        "        elif self.unk2idx is not None:\n",
        "            return self.unk2idx\n",
        "        else:\n",
        "            raise ValueError('word %s not in dictionary (while dictionary does not contain <unk>)' % w)\n",
        "\n",
        "    def tokenize_and_index(self, sentence):\n",
        "        inds = [self.word2idx(w) for w in tokenize(sentence)]\n",
        "        return "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset personalizados"
      ],
      "metadata": {
        "id": "MPsWq3VN_EQx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset BoW"
      ],
      "metadata": {
        "id": "1JxNxL58_H75"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MW1WiVl7IxVL"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from skimage import io\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "class VQA_BOW(Dataset):\n",
        "  def __init__(self, questions_path: str, annotations_path: str, images_folder_path: str, transform ,max_qst_length=30, max_num_ans=10):\n",
        "    super(VQA_BOW, self).__init__()\n",
        "    self.qn_path = questions_path\n",
        "    self.an_path = annotations_path\n",
        "    self.img_path = images_folder_path\n",
        "\n",
        "    self.transform = transform\n",
        "\n",
        "    top_questionsBOW,  qst_bow_dict, preguntas = make_bow_qst_vocabulary(\"Questions\",1024)\n",
        "    top_answersBOW,  ans_bow_dict = make_bow_ans_vocabulary(\"Annotations\",1000)\n",
        "\n",
        "    self.top_answers = top_answersBOW\n",
        "\n",
        "    qst_bow_table = lookUpBOW(top_questionsBOW,  qst_bow_dict, preguntas)\n",
        "    ans_bow_table = lookUpBOW(top_answersBOW,  ans_bow_dict , top_answersBOW)\n",
        "\n",
        "    self.qst_bow_table = qst_bow_table\n",
        "    self.ans_bow_table = ans_bow_table\n",
        "\n",
        "    self.max_qst_length = max_qst_length\n",
        "    self.max_num_ans = max_num_ans\n",
        "\n",
        "    self.ann_dataset = json.load(open(self.an_path, 'r'))['annotations']\n",
        "    qn_json = json.load(open(self.qn_path, 'r'))\n",
        "    self.qn_dataset = qn_json['questions']\n",
        "    self.img_prefix = f\"{qn_json['data_type']}_{qn_json['data_subtype']}\"\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.qn_dataset)\n",
        "\n",
        "  def __getitem__(self, item: int):\n",
        "\n",
        "    qst_bow_table = self.qst_bow_table\n",
        "    ans_bow_table = self.ans_bow_table\n",
        "    max_qst_length = self.max_qst_length\n",
        "    max_num_ans = self.max_num_ans\n",
        "\n",
        "    \n",
        "    question = self.qn_dataset[item]\n",
        "    question_str = question['question']\n",
        "\n",
        "    qst_vec = qst_bow_table[item]\n",
        "\n",
        "    choices = question['multiple_choices']\n",
        "\n",
        "    answer = self.ann_dataset[item]['multiple_choice_answer']\n",
        "    \n",
        "    ans2idx = 0\n",
        "    if answer in self.top_answers:\n",
        "        #indice de la respuesta\n",
        "        ans2idx = self.top_answers.index(answer)\n",
        "\n",
        "    image = Image.open(os.path.join(self.img_path, f\"{self.img_prefix}_{question['image_id']:012d}.png\")).convert('RGB')\n",
        "    if self.transform:\n",
        "      image = self.transform(image)    \n",
        "    \n",
        "    return image, question_str, choices, answer, qst_vec, ans2idx\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset LSTM"
      ],
      "metadata": {
        "id": "bsxRvUUO_FfR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptV54Xr01frn"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from skimage import io\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "class VQA_LSTM(Dataset):\n",
        "  def __init__(self, questions_path: str, annotations_path: str, images_folder_path: str, transform ,max_qst_length=30, max_num_ans=10):\n",
        "    super(VQA_LSTM, self).__init__()\n",
        "    self.qn_path = questions_path\n",
        "    self.an_path = annotations_path\n",
        "    self.img_path = images_folder_path\n",
        "\n",
        "    self.transform = transform\n",
        "\n",
        "    #retorna una lista con un vocabulario de preguntas\n",
        "    vocab_list, _ = make_vocab_questions('Questions')\n",
        "    #retorna una lista con el top 1000 respuestas\n",
        "    top_answers, _ = make_vocab_answers('Annotations',1000)\n",
        "\n",
        "    #Diccionario de preguntas\n",
        "    self.qst_vocab = VocabDict(vocab_list)\n",
        "    #Diccionario de respuestas\n",
        "    self.ans_vocab = VocabDict(top_answers)\n",
        "\n",
        "    self.max_qst_length = max_qst_length\n",
        "    self.max_num_ans = max_num_ans\n",
        "\n",
        "    self.ann_dataset = json.load(open(self.an_path, 'r'))['annotations']\n",
        "    qn_json = json.load(open(self.qn_path, 'r'))\n",
        "    self.qn_dataset = qn_json['questions']\n",
        "    self.img_prefix = f\"{qn_json['data_type']}_{qn_json['data_subtype']}\"\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.qn_dataset)\n",
        "\n",
        "  def __getitem__(self, item: int):\n",
        "\n",
        "    qst_vocab = self.qst_vocab\n",
        "    ans_vocab = self.ans_vocab\n",
        "    max_qst_length = self.max_qst_length\n",
        "    max_num_ans = self.max_num_ans\n",
        "\n",
        "    \n",
        "    question = self.qn_dataset[item]\n",
        "    question_str = question['question']\n",
        "\n",
        "    qst2idc = np.array([qst_vocab.word2idx('<pad>')] * max_qst_length)  # padded with '<pad>' in 'ans_vocab'\n",
        "    qst2idc[:len(tokenize(question['question']))] = [qst_vocab.word2idx(w) for w in tokenize(question['question'])]\n",
        "\n",
        "    choices = question['multiple_choices']\n",
        "\n",
        "    answer = self.ann_dataset[item]['multiple_choice_answer']\n",
        "    ans2idx = ans_vocab.word2idx(answer)\n",
        "\n",
        "    image = Image.open(os.path.join(self.img_path, f\"{self.img_prefix}_{question['image_id']:012d}.png\")).convert('RGB')\n",
        "    if self.transform:\n",
        "      image = self.transform(image)    \n",
        "\n",
        "    return image, question_str, choices, answer, qst2idc, ans2idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-D91obs_a__7",
        "outputId": "b0166729-d646-4794-c6b0-ea991f0c0c6e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17, 'on grass')"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "# trabajo con listas\n",
        "L = ['2', 'yes', 'man on table', 'out on porch', 'cotton', '1', '3', 'blue', 'white', 'brown', 'no', 'yard', 'sandals', 'on deck', '4', 'pot', 'red', 'on grass']\n",
        "L.index('on grass'),L[17]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeF_OoJmoi5j",
        "outputId": "2bf350cd-2a0b-4163-b7e6-93b67ffc3261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Se eliminan los signos de puntuación:  !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
            "Who looks happier?\n",
            "Is the dog asleep?\n",
            "Estamos haciendo un vocabulario con todas las palabras de todas las preguntas\n",
            "El número total de palabras en las preguntas son: 5346\n",
            "El largo máximo de una pregunta es: 21\n",
            "Make vocabulary for answers\n",
            "The number of total words of answers: 24058\n",
            "Keep top 1000 answers into vocab\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "if not bow:\n",
        "    questions_path='Questions'\n",
        "    annotation_path='Annotations'\n",
        "\n",
        "    #generamos los archivos que despues se utilizan para generar el diccionario de preguntas\n",
        "    qvocab_list, preguntas = make_vocab_questions(questions_path)\n",
        "    #generamos los archivos que despues se utilizan para generar el diccionario de respuestas\n",
        "    top_answers, answers = make_vocab_answers(annotation_path,1000)\n",
        "\n",
        "    # Uso\n",
        "    dataset = VQA_LSTM(questions_path='Questions/MultipleChoice_abstract_v002_val2015_questions.json', \n",
        "                annotations_path='Annotations/abstract_v002_val2015_annotations.json',\n",
        "                images_folder_path='Images/val/', transform=transforms.Compose([\n",
        "                                                        transforms. ToTensor(),\n",
        "                                                        transforms.Resize((224,224))\n",
        "                                                    ]))\n",
        "\n",
        "    # Tomar muestra aleatoria\n",
        "    image, question, choices, answer, qst2idc, anslabel = dataset[50]\n",
        "\n",
        "    print(\"Dimension de la imagen:\",image.shape)\n",
        "    print(question,qst2idc)\n",
        "    print(\"Alternativas:\",choices)\n",
        "    print(\"Indice de la solución\",anslabel)\n",
        "\n",
        "\n",
        "    # Visualizar\n",
        "    img = image.numpy().transpose((1, 2, 0))\n",
        "    plt.imshow(img)\n",
        "    plt.suptitle(f\"{question}\")\n",
        "    plt.title(f\"{answer}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "EImc7bz2Meu2",
        "outputId": "7b4c8925-483a-4502-8bdd-e321f2313f0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimension de la imagen: torch.Size([3, 224, 224])\n",
            "Is the grass green? [0. 0. 2. ... 0. 0. 0.]\n",
            "Alternativas: ['exploring', 'no idea', 'to eat', '1', 'white', 'coat', 'not sure', 'red', '3', 'blue', 'football, soccer', '2', \"she's playing with her cat\", 'yes', 'no', '4', 'for fun', 'brown']\n",
            "Indice de la solución 0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "if bow:\n",
        "    # Uso\n",
        "    dataset = VQA_BOW(questions_path='Questions/MultipleChoice_abstract_v002_val2015_questions.json', \n",
        "                annotations_path='Annotations/abstract_v002_val2015_annotations.json',\n",
        "                images_folder_path='Images/val/', transform=transforms.Compose([\n",
        "                                                        transforms. ToTensor(),\n",
        "                                                        transforms.Resize((224,224))\n",
        "                                                    ]))\n",
        "\n",
        "    # Tomar muestra aleatoria\n",
        "    image, question, choices, answer, qst2idc, anslabel = dataset[50]\n",
        "\n",
        "    print(\"Dimension de la imagen:\",image.shape)\n",
        "    print(question,qst2idc)\n",
        "    print(\"Alternativas:\",choices)\n",
        "    print(\"Indice de la solución\",anslabel)\n",
        "    #print(\"Solución BOW\",top_answersBOW[anslabel])\n",
        "\n",
        "    # Visualizar\n",
        "    img = image.numpy().transpose((1, 2, 0))\n",
        "    plt.imshow(img)\n",
        "    plt.suptitle(f\"{question}\")\n",
        "    plt.title(f\"{answer}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJb-leLNEvoo",
        "outputId": "569d9818-da86-4a88-e652-f294cebec906"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['<unk>', 'yes', 'no', '2', '1'], 1000)"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "#El indice de yes es el 1, entonces de las mil salidas la neurona con mayor probailidad deberia ser esa\n",
        "top_answers[0:5], len(top_answers)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generando los dataset\n",
        "\n",
        "Se generan los dataset para cada modelo."
      ],
      "metadata": {
        "id": "YsH9WA-zB_-B"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4k2hpkZY-m6Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70db6a0e-ebf4-4006-edfe-02d10724dd16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Se eliminan los signos de puntuación:  !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
            "Who looks happier?\n",
            "Is the dog asleep?\n",
            "Estamos haciendo un vocabulario con todas las palabras de todas las preguntas\n",
            "El número total de palabras en las preguntas son: 5346\n",
            "El largo máximo de una pregunta es: 21\n",
            "Make vocabulary for answers\n",
            "The number of total words of answers: 24058\n",
            "Keep top 1000 answers into vocab\n",
            "Se eliminan los signos de puntuación:  !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
            "Who looks happier?\n",
            "Is the dog asleep?\n",
            "Estamos haciendo un vocabulario con todas las palabras de todas las preguntas\n",
            "El número total de palabras en las preguntas son: 5346\n",
            "El largo máximo de una pregunta es: 21\n",
            "Make vocabulary for answers\n",
            "The number of total words of answers: 24058\n",
            "Keep top 1000 answers into vocab\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 64\n",
        "if bow == True:\n",
        "    trainDatasetBOW = VQA_BOW(questions_path='Questions/MultipleChoice_abstract_v002_train2015_questions.json', \n",
        "              annotations_path='Annotations/abstract_v002_train2015_annotations.json',\n",
        "              images_folder_path='Images/train/',  transform=transforms.Compose([\n",
        "                                                    transforms. ToTensor(),\n",
        "                                                    transforms.Resize((224,224))\n",
        "                                                ]))\n",
        "    valDatasetBOW   = VQA_BOW(questions_path='Questions/MultipleChoice_abstract_v002_val2015_questions.json', \n",
        "                annotations_path='Annotations/abstract_v002_val2015_annotations.json',\n",
        "                images_folder_path='Images/val/',  transform=transforms.Compose([\n",
        "                                                        transforms. ToTensor(),\n",
        "                                                        transforms.Resize((224,224))\n",
        "                                                    ]))\n",
        "    train_loader = torch.utils.data.DataLoader(trainDataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = torch.utils.data.DataLoader(valDataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "else:\n",
        "    trainDataset = VQA_LSTM(questions_path='Questions/MultipleChoice_abstract_v002_train2015_questions.json', \n",
        "                annotations_path='Annotations/abstract_v002_train2015_annotations.json',\n",
        "                images_folder_path='Images/train/',  transform=transforms.Compose([\n",
        "                                                        transforms. ToTensor(),\n",
        "                                                        transforms.Resize((224,224))\n",
        "                                                    ]))\n",
        "    valDataset   = VQA_LSTM(questions_path='Questions/MultipleChoice_abstract_v002_val2015_questions.json', \n",
        "                annotations_path='Annotations/abstract_v002_val2015_annotations.json',\n",
        "                images_folder_path='Images/val/',  transform=transforms.Compose([\n",
        "                                                        transforms. ToTensor(),\n",
        "                                                        transforms.Resize((224,224))\n",
        "                                                    ]))\n",
        "    train_loaderBOW = torch.utils.data.DataLoader(trainDatasetBOW, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
        "    val_loaderBOW = torch.utils.data.DataLoader(valDatasetBOW, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generando modelos\n",
        "\n"
      ],
      "metadata": {
        "id": "O6O4GX13C4ny"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISTu1_H3PZOf"
      },
      "source": [
        "## Primer modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZIQYSJpPZbO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "class ImgEncoderBOW(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size):\n",
        "        \"\"\"Red VGG19 preentrenada con los pesos de imagenet\n",
        "        \"\"\"\n",
        "        super(ImgEncoderBOW, self).__init__()\n",
        "        model = models.vgg19(pretrained=True)\n",
        "        in_features = model.classifier[-1].in_features  # input size of feature vector\n",
        "        model.classifier = nn.Sequential(*list(model.classifier.children())[:-1])    # remove last fc layer\n",
        "\n",
        "        self.model = model                              # loaded model without last fc layer\n",
        "        self.fc = nn.Linear(in_features, embed_size)    # feature vector of image\n",
        "\n",
        "    def forward(self, image):\n",
        "        \"\"\"Extract feature vector from image vector.\n",
        "        \"\"\"\n",
        "        #image = image.permute(0, 3, 1, 2)\n",
        "        with torch.no_grad():\n",
        "            img_feature = self.model(image)                  # [batch_size, vgg16(19)_fc=4096]\n",
        "        img_feature = self.fc(img_feature)                   # [batch_size, embed_size]\n",
        "\n",
        "        l2_norm = img_feature.norm(p=2, dim=1, keepdim=True).detach()\n",
        "        img_feature = img_feature.div(l2_norm)               # l2-normalized feature vector\n",
        "\n",
        "        return img_feature\n",
        "\n",
        "class VqaModelBOW(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size, qst_vocab_size, word_embed_size, num_layers, hidden_size):\n",
        "\n",
        "        super(VqaModelBOW, self).__init__()\n",
        "        self.img_encoder = ImgEncoderBOW(embed_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(embed_size, 1000)\n",
        "        self.fc2 = nn.Linear(1000, 1000)\n",
        "\n",
        "    def forward(self, img, qst_feature):\n",
        "        #se le pasa de forma directa el BoW encoding\n",
        "        img_feature = self.img_encoder(img)                    \n",
        "        combined_feature = torch.mul(img_feature, qst_feature)  \n",
        "        combined_feature = self.tanh(combined_feature)\n",
        "        combined_feature = self.dropout(combined_feature)\n",
        "        combined_feature = self.fc1(combined_feature)          \n",
        "        combined_feature = self.tanh(combined_feature)\n",
        "        combined_feature = self.dropout(combined_feature)\n",
        "        combined_feature = self.fc2(combined_feature)        \n",
        "\n",
        "        return combined_feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip2KEpjJpWC_"
      },
      "source": [
        "## Segundo modelo "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXsGA_wMoT2j"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "class ImgEncoder(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size):\n",
        "        \"\"\"Feature vector de la VGG19 preentrenada con los pesos de imagenet\n",
        "        \"\"\"\n",
        "        super(ImgEncoder, self).__init__()\n",
        "        model = models.vgg19(pretrained=True)\n",
        "        in_features = model.classifier[-1].in_features  # input size of feature vector\n",
        "        model.classifier = nn.Sequential(*list(model.classifier.children())[:-1])    # remove last fc layer\n",
        "\n",
        "        self.model = model                              # loaded model without last fc layer\n",
        "        self.fc = nn.Linear(in_features, embed_size)    # feature vector of image 4096->1000\n",
        "\n",
        "    def forward(self, image):\n",
        "        \"\"\"Se extraen las caracteristicas\n",
        "        \"\"\"\n",
        "        #image = image.permute(0, 3, 1, 2)\n",
        "        with torch.no_grad():\n",
        "            img_feature = self.model(image)            \n",
        "        img_feature = self.fc(img_feature)                   \n",
        "\n",
        "        l2_norm = img_feature.norm(p=2, dim=1, keepdim=True).detach()\n",
        "        img_feature = img_feature.div(l2_norm)          # se normaliza\n",
        "\n",
        "        return img_feature\n",
        "\n",
        "\n",
        "class QstEncoder(nn.Module):\n",
        "    \"\"\"Red neuronal recurrente LSTM\"\"\"\n",
        "\n",
        "    def __init__(self, qst_vocab_size, word_embed_size, embed_size, num_layers, hidden_size):\n",
        "\n",
        "        super(QstEncoder, self).__init__()\n",
        "        self.word2vec = nn.Embedding(qst_vocab_size, word_embed_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.lstm = nn.LSTM(word_embed_size, hidden_size, num_layers)\n",
        "        self.fc = nn.Linear(2*num_layers*hidden_size, embed_size)   \n",
        "        #LSTM con salida de tamaño 1024\n",
        "        \n",
        "    def forward(self, question):\n",
        "\n",
        "        qst_vec = self.word2vec(question)                             \n",
        "        qst_vec = self.tanh(qst_vec)\n",
        "        qst_vec = qst_vec.transpose(0, 1)                             \n",
        "        _, (hidden, cell) = self.lstm(qst_vec)                      \n",
        "        qst_feature = torch.cat((hidden, cell), 2)                    \n",
        "        qst_feature = qst_feature.transpose(0, 1)                  \n",
        "        qst_feature = qst_feature.reshape(qst_feature.size()[0], -1) \n",
        "        qst_feature = self.tanh(qst_feature)\n",
        "        qst_feature = self.fc(qst_feature)                            \n",
        "\n",
        "        return qst_feature\n",
        "\n",
        "\n",
        "class VqaModel(nn.Module):\n",
        "\n",
        "    def __init__(self, embed_size, qst_vocab_size, word_embed_size, num_layers, hidden_size):\n",
        "\n",
        "        super(VqaModel, self).__init__()\n",
        "        self.img_encoder = ImgEncoder(embed_size)\n",
        "        self.qst_encoder = QstEncoder(qst_vocab_size, word_embed_size, embed_size, num_layers, hidden_size)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(embed_size, 1000)\n",
        "        self.fc2 = nn.Linear(1000, 1000)\n",
        "\n",
        "    def forward(self, img, qst):\n",
        "        img_feature = self.img_encoder(img)                     \n",
        "        qst_feature = self.qst_encoder(qst)                     \n",
        "        combined_feature = torch.mul(img_feature, qst_feature)  \n",
        "        combined_feature = self.tanh(combined_feature)\n",
        "        combined_feature = self.dropout(combined_feature)\n",
        "        combined_feature = self.fc1(combined_feature)          \n",
        "        combined_feature = self.tanh(combined_feature)\n",
        "        combined_feature = self.dropout(combined_feature)\n",
        "        combined_feature = self.fc2(combined_feature)      #salida de 1000    \n",
        "\n",
        "        return combined_feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RR0fFeBeNrpC"
      },
      "outputs": [],
      "source": [
        "if not bow:\n",
        "    qst_vocab_size = train_loader.dataset.qst_vocab.vocab_size\n",
        "    ans_vocab_size = 1000\n",
        "embed_size = 1024\n",
        "word_embed_size = 300\n",
        "num_layers = 2\n",
        "hidden_size = 512\n",
        "learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6RKLiVF8aKT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "924dc53a-af7d-4590-889c-e53f1ac40eeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño de los vocabularios, question vocab size:  5348 answer vocab size 1000\n"
          ]
        }
      ],
      "source": [
        "print(\"Tamaño de los vocabularios, question vocab size: \",qst_vocab_size,\"answer vocab size\",ans_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2pLDp0sQMyd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cf4bce0-2f86-4237-af9c-e350bbd21b40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000 30000\n"
          ]
        }
      ],
      "source": [
        "print(len(train_loader.dataset),len(val_loader.dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "924dc53a-af7d-4590-889c-e53f1ac40eeb",
        "id": "UenU2kIqG2-W"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño de los vocabularios, question vocab size:  5348 answer vocab size 1000\n"
          ]
        }
      ],
      "source": [
        "print(\"Tamaño de los vocabularios, question vocab size: \",qst_vocab_size,\"answer vocab size\",ans_vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cf4bce0-2f86-4237-af9c-e350bbd21b40",
        "id": "Q8WMeOgmG2-W"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000 30000\n"
          ]
        }
      ],
      "source": [
        "print(len(train_loader.dataset),len(val_loader.dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I45NVmR9qxYO"
      },
      "source": [
        "# Train function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Device\n",
        "\n",
        "Se utiliza el entorno GPU."
      ],
      "metadata": {
        "id": "Rx-PTjt4G5kS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09ztpjCJBK6o",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "829e3396-e859-476f-cfc1-7b6c838ae44b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "device = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Función Train para el primer modelo\n",
        "\n",
        "En base a los modelos de la tarea 5 y del curso Deep learning."
      ],
      "metadata": {
        "id": "JqX6ebvuG_u6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOLul3VFQeKX"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import copy\n",
        "\n",
        "def trainBOW(net, optimizer, num_epocas):\n",
        "    inicio = time.time()\n",
        "\n",
        "    #copiamos el modelo utilizando la libreria copy\n",
        "    best_model_wts = copy.deepcopy(net.state_dict()) \n",
        "    train_losses, train_counter, train_accuracy, val_losses,val_accuracy,val_counter  = [],[],[],[],[],[]\n",
        "    \n",
        "    best_acc = 0.0\n",
        "    best_loss = 2e32\n",
        "    for epoch in range(num_epocas):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epocas-1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        net.train() #Modo entrenamiento\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0.0\n",
        "        batch_size = 64\n",
        "        batch_step_size = len(train_loaderBOW.dataset) / batch_size\n",
        "        a=0\n",
        "        for batch_idx, batch_sample in enumerate(train_loaderBOW):\n",
        "            image = batch_sample[0].to(device).float()\n",
        "            question = batch_sample[4].to(device).float()\n",
        "            label = batch_sample[5].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            if a==0:\n",
        "              print(image.shape,question.shape,label.shape)\n",
        "              print(\"question\",question[0])\n",
        "              print(\"label\",label[0])\n",
        "              a+=1\n",
        "\n",
        "            output = net(image,question) #salidas de la red\n",
        "            pred_indices = torch.argmax(output,1)  # [batch_size]\n",
        "            loss = criterion(output, label)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            running_corrects += torch.sum(pred_indices == label.data)\n",
        "            if batch_idx %100 == 0:\n",
        "                print('Batch idx: ',batch_idx,' Batch Step: ' ,int(batch_step_size)) #para ver el avance\n",
        "\n",
        "        epoch_loss = running_loss /len(train_loaderBOW.dataset) #promedio de error \n",
        "        epoch_acc = running_corrects / len(train_loaderBOW.dataset) #promedio de accuracy\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_counter.append(epoch)\n",
        "        train_accuracy.append(epoch_acc)\n",
        "\n",
        "        print('Train Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
        "\n",
        "        #Validacion \n",
        "        net.eval()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0.0\n",
        "        for batch_idx, batch_sample in enumerate(val_loaderBOW):\n",
        "            image = batch_sample[0].to(device).float()\n",
        "            question = batch_sample[4].to(device).float()\n",
        "            label = batch_sample[5].to(device)\n",
        "            with torch.set_grad_enabled(False):\n",
        "                output = net(image,question) #salidas de la red\n",
        "                _, pred_indices = torch.max(output, 1)  # [batch_size]\n",
        "                loss = criterion(output, label)\n",
        " \n",
        "                running_loss += loss.item()\n",
        "                running_corrects += torch.sum(pred_indices == label)\n",
        "        \n",
        "        epoch_loss = running_loss /len(val_loaderBOW.dataset) #promedio de error \n",
        "        epoch_acc = running_corrects / len(val_loaderBOW.dataset) #promedio de accuracy\n",
        "        val_losses.append(epoch_loss)\n",
        "        val_counter.append(epoch)\n",
        "        val_accuracy.append(epoch_acc)\n",
        "        #chekpoint\n",
        "        if epoch_loss < best_loss:\n",
        "            best_loss = epoch_loss\n",
        "            best_model_wts = copy.deepcopy(net.state_dict())\n",
        "        \n",
        "        # early stopping, si el error aumenta más de 5 veces respecto al menor error,\n",
        "        # terminamos el entrenamiento\n",
        "        if epoch_loss > best_loss*5:\n",
        "            print('\\n'+'-' * 10+'Early Stopping'+'-' * 10+'\\n')\n",
        "            break\n",
        "\n",
        "    print('Best val loss: {:.4f}'.format(best_loss))\n",
        "\n",
        "    final = time.time()\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format((final-inicio)//60, (final-inicio) % 60))\n",
        "\n",
        "    net.load_state_dict(best_model_wts)\n",
        "    return net, train_counter,train_losses,train_accuracy,val_counter,val_losses,val_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Función train para el segundo modelo"
      ],
      "metadata": {
        "id": "8FfbSXF-HYUD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gKWUup31qw_j"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import copy\n",
        "\n",
        "def train(net, optimizer, num_epocas):\n",
        "    inicio = time.time()\n",
        "\n",
        "    ans_unk_idx = train_loader.dataset.ans_vocab.unk2idx\n",
        "\n",
        "    #copiamos el modelo utilizando la libreria copy\n",
        "    best_model_wts = copy.deepcopy(net.state_dict()) \n",
        "    train_losses, train_counter, train_accuracy, val_losses,val_accuracy,val_counter  = [],[],[],[],[],[]\n",
        "    \n",
        "    best_acc = 0.0\n",
        "    best_loss = 2e32\n",
        "    for epoch in range(num_epocas):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epocas-1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        net.train() #Modo entrenamiento\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0.0\n",
        "        batch_size = 64\n",
        "        batch_step_size = len(train_loader.dataset) / batch_size\n",
        "        for batch_idx, batch_sample in enumerate(train_loader):\n",
        "            image = batch_sample[0].to(device).float()\n",
        "            question = batch_sample[4].to(device).float()\n",
        "            label = batch_sample[5].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = net(image,question) #salidas de la red\n",
        "            pred_indices = torch.argmax(output,1)  # [batch_size]\n",
        "            loss = criterion(output, label)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            #castigamos en caso de que la respuesta sea desconocida\n",
        "            pred_indices[pred_indices == ans_unk_idx] = -9999\n",
        "            running_loss += loss.item()\n",
        "            running_corrects += torch.sum(pred_indices == label.data)\n",
        "            if batch_idx %100 == 0:\n",
        "                print('Batch idx: ',batch_idx,' Batch Step: ' ,int(batch_step_size))\n",
        "\n",
        "        epoch_loss = running_loss /len(train_loader.dataset) #promedio de error \n",
        "        epoch_acc = running_corrects / len(train_loader.dataset) #promedio de accuracy\n",
        "        train_losses.append(epoch_loss)\n",
        "        train_counter.append(epoch)\n",
        "        train_accuracy.append(epoch_acc)\n",
        "\n",
        "        print('Train Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
        "\n",
        "        #Validacion \n",
        "        net.eval()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_corrects = 0.0\n",
        "        for batch_idx, batch_sample in enumerate(val_loader):\n",
        "            image = batch_sample[0].to(device).float()\n",
        "            question = batch_sample[4].to(device).float()\n",
        "            label = batch_sample[5].to(device)\n",
        "            with torch.set_grad_enabled(False):\n",
        "                output = net(image,question) #salidas de la red\n",
        "                _, pred_indices = torch.max(output, 1)  # [batch_size]\n",
        "                loss = criterion(output, label)\n",
        " \n",
        "                #castigamos en caso de que la respuesta sea desconocida\n",
        "                pred_indices[pred_indices == ans_unk_idx] = -9999\n",
        "                running_loss += loss.item()\n",
        "                running_corrects += torch.sum(pred_indices == label)\n",
        "        \n",
        "        epoch_loss = running_loss /len(val_loader.dataset) #promedio de error \n",
        "        epoch_acc = running_corrects / len(val_loader.dataset) #promedio de accuracy\n",
        "        val_losses.append(epoch_loss)\n",
        "        val_counter.append(epoch)\n",
        "        val_accuracy.append(epoch_acc)\n",
        "        #chekpoint\n",
        "        if epoch_loss < best_loss:\n",
        "            best_loss = epoch_loss\n",
        "            best_model_wts = copy.deepcopy(net.state_dict())\n",
        "        \n",
        "        # early stopping, si el error aumenta más de 5 veces respecto al menor error,\n",
        "        # terminamos el entrenamiento\n",
        "        if epoch_loss > best_loss*5:\n",
        "            print('\\n'+'-' * 10+'Early Stopping'+'-' * 10+'\\n')\n",
        "            break\n",
        "\n",
        "    print('Best val loss: {:.4f}'.format(best_loss))\n",
        "\n",
        "    final = time.time()\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format((final-inicio)//60, (final-inicio) % 60))\n",
        "\n",
        "    net.load_state_dict(best_model_wts)\n",
        "    return net, train_counter,train_losses,train_accuracy,val_counter,val_losses,val_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inicializando los modelos\n",
        "\n",
        "Los demás parámetros se utilizan para generar el modelo del paper. \n",
        "\n",
        "* Utilizando Entropia cruzada.\n",
        "* Adam optimizer.\n",
        "* Bath Size de 64.\n",
        "* Learning rate de 0.001 (tambien se realizaron experimentos variando este valor)."
      ],
      "metadata": {
        "id": "RmLQILejHvRa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FSUagpr_7vG"
      },
      "outputs": [],
      "source": [
        "if BOW == True:\n",
        "    net = VqaModelBOW(embed_size, qst_vocab_size, ans_vocab_size, word_embed_size, num_layers, hidden_size)\n",
        "    net.cuda()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate) \n",
        "    \n",
        "else:\n",
        "    net = VqaModel(embed_size, qst_vocab_size, ans_vocab_size, word_embed_size, num_layers, hidden_size)\n",
        "    net.cuda()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate) \n",
        "    model, train_counter,train_loss,train_accuracy,val_counter,val_loss,val_accuracy = train(net, optimizer, num_epocas=6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Entrenamos\n",
        "\n",
        "Se utilizan 6 epocas solamente debido al alto tiempo que toma entrenar."
      ],
      "metadata": {
        "id": "4payD4HTIyJb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xx5oiq69YGe9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0eb22481-4d54-479a-84a7-b79cc39229bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/5\n",
            "----------\n",
            "torch.Size([64, 3, 224, 224]) torch.Size([64, 1024]) torch.Size([64])\n",
            "question tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
            "label tensor(0, device='cuda:0')\n",
            "Batch idx:  0  Batch Step:  937\n",
            "Batch idx:  100  Batch Step:  937\n",
            "Batch idx:  200  Batch Step:  937\n",
            "Batch idx:  300  Batch Step:  937\n",
            "Batch idx:  400  Batch Step:  937\n",
            "Batch idx:  500  Batch Step:  937\n",
            "Batch idx:  600  Batch Step:  937\n",
            "Batch idx:  700  Batch Step:  937\n",
            "Batch idx:  800  Batch Step:  937\n",
            "Batch idx:  900  Batch Step:  937\n",
            "Train Loss: 0.0536 Acc: 0.3191\n",
            "Epoch 1/5\n",
            "----------\n",
            "torch.Size([64, 3, 224, 224]) torch.Size([64, 1024]) torch.Size([64])\n",
            "question tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0')\n",
            "label tensor(2, device='cuda:0')\n",
            "Batch idx:  0  Batch Step:  937\n",
            "Batch idx:  100  Batch Step:  937\n",
            "Batch idx:  200  Batch Step:  937\n",
            "Batch idx:  300  Batch Step:  937\n",
            "Batch idx:  400  Batch Step:  937\n",
            "Batch idx:  500  Batch Step:  937\n",
            "Batch idx:  600  Batch Step:  937\n",
            "Batch idx:  700  Batch Step:  937\n",
            "Batch idx:  800  Batch Step:  937\n",
            "Batch idx:  900  Batch Step:  937\n",
            "Train Loss: 0.0476 Acc: 0.3450\n",
            "Epoch 2/5\n",
            "----------\n",
            "torch.Size([64, 3, 224, 224]) torch.Size([64, 1024]) torch.Size([64])\n",
            "question tensor([0., 0., 1.,  ..., 0., 0., 0.], device='cuda:0')\n",
            "label tensor(44, device='cuda:0')\n",
            "Batch idx:  0  Batch Step:  937\n",
            "Batch idx:  100  Batch Step:  937\n",
            "Batch idx:  200  Batch Step:  937\n",
            "Batch idx:  300  Batch Step:  937\n",
            "Batch idx:  400  Batch Step:  937\n",
            "Batch idx:  500  Batch Step:  937\n",
            "Batch idx:  600  Batch Step:  937\n",
            "Batch idx:  700  Batch Step:  937\n",
            "Batch idx:  800  Batch Step:  937\n",
            "Batch idx:  900  Batch Step:  937\n",
            "Train Loss: 0.0456 Acc: 0.3548\n",
            "Epoch 3/5\n",
            "----------\n",
            "torch.Size([64, 3, 224, 224]) torch.Size([64, 1024]) torch.Size([64])\n",
            "question tensor([0., 0., 1.,  ..., 0., 0., 0.], device='cuda:0')\n",
            "label tensor(5, device='cuda:0')\n",
            "Batch idx:  0  Batch Step:  937\n",
            "Batch idx:  100  Batch Step:  937\n",
            "Batch idx:  200  Batch Step:  937\n",
            "Batch idx:  300  Batch Step:  937\n",
            "Batch idx:  400  Batch Step:  937\n",
            "Batch idx:  500  Batch Step:  937\n",
            "Batch idx:  600  Batch Step:  937\n",
            "Batch idx:  700  Batch Step:  937\n",
            "Batch idx:  800  Batch Step:  937\n",
            "Batch idx:  900  Batch Step:  937\n",
            "Train Loss: 0.0441 Acc: 0.3643\n",
            "Epoch 4/5\n",
            "----------\n",
            "torch.Size([64, 3, 224, 224]) torch.Size([64, 1024]) torch.Size([64])\n",
            "question tensor([0., 0., 1.,  ..., 0., 0., 0.], device='cuda:0')\n",
            "label tensor(0, device='cuda:0')\n",
            "Batch idx:  0  Batch Step:  937\n",
            "Batch idx:  100  Batch Step:  937\n",
            "Batch idx:  200  Batch Step:  937\n",
            "Batch idx:  300  Batch Step:  937\n",
            "Batch idx:  400  Batch Step:  937\n",
            "Batch idx:  500  Batch Step:  937\n",
            "Batch idx:  600  Batch Step:  937\n",
            "Batch idx:  700  Batch Step:  937\n",
            "Batch idx:  800  Batch Step:  937\n",
            "Batch idx:  900  Batch Step:  937\n",
            "Train Loss: 0.0427 Acc: 0.3717\n",
            "Epoch 5/5\n",
            "----------\n",
            "torch.Size([64, 3, 224, 224]) torch.Size([64, 1024]) torch.Size([64])\n",
            "question tensor([0., 0., 1.,  ..., 0., 0., 0.], device='cuda:0')\n",
            "label tensor(8, device='cuda:0')\n",
            "Batch idx:  0  Batch Step:  937\n",
            "Batch idx:  100  Batch Step:  937\n",
            "Batch idx:  200  Batch Step:  937\n",
            "Batch idx:  300  Batch Step:  937\n",
            "Batch idx:  400  Batch Step:  937\n",
            "Batch idx:  500  Batch Step:  937\n",
            "Batch idx:  600  Batch Step:  937\n",
            "Batch idx:  700  Batch Step:  937\n",
            "Batch idx:  800  Batch Step:  937\n",
            "Batch idx:  900  Batch Step:  937\n",
            "Train Loss: 0.0418 Acc: 0.3755\n",
            "Best val loss: 0.0698\n",
            "Training complete in 108m 16s\n"
          ]
        }
      ],
      "source": [
        "if BOW == True:\n",
        "    model, train_counter,train_loss,train_accuracy,val_counter,val_loss,val_accuracy = trainBOW(net, optimizer, num_epocas=6)\n",
        "else:\n",
        "    model, train_counter,train_loss,train_accuracy,val_counter,val_loss,val_accuracy = train(net, optimizer, num_epocas=6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJb4l5hMVGyn"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "#2b. Graficar las curvas de loss de entrenamiento y validación\n",
        "plt.title(f\"Loss curve Lr={learning_rate}\")\n",
        "plt.plot(train_counter, train_loss, label='Entrenamiento',color='blue')\n",
        "plt.plot(val_counter,val_loss, label='Validacion',color='red')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "train_accuracy_list = []\n",
        "for x in train_accuracy:\n",
        "  train_accuracy_list.append(x.cpu())\n",
        "\n",
        "val_accuracy_list = []\n",
        "for x in val_accuracy:\n",
        "  val_accuracy_list.append(x.cpu())\n",
        "\n",
        "plt.figure()\n",
        "plt.title(f\"Model Accuracy Lr={learning_rate}\")\n",
        "plt.plot(train_counter, train_accuracy_list, label='Entrenamiento',color='blue')\n",
        "plt.plot(val_counter,val_accuracy_list, label='Validacion',color='red')\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teHrH9Z0Qybv"
      },
      "outputs": [],
      "source": [
        "print(\"Mayor accuracy en entrenamiento:\",max(train_accuracy_list), \"\\nMayor accuracy en validación\",max(val_accuracy_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScFfpCk2Qybv"
      },
      "outputs": [],
      "source": [
        "print(\"Menor loss en entrenamiento:\",min(train_loss), \"\\nMenor loss en validación\",min(val_loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWBLRLT3ErM_"
      },
      "source": [
        "# Visualizando los resultados en el conjunto de validación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmC1n2q-7smw"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "if BOW==True:\n",
        "    val_loader = torch.utils.data.DataLoader(valDataset, batch_size=1, num_workers=2, pin_memory=True)\n",
        "\n",
        "    for batch_idx, batch_sample in enumerate(val_loader):\n",
        "        if batch_idx<10:\n",
        "            image = batch_sample[0].to(device).float()\n",
        "            question = batch_sample[4].to(device)\n",
        "            label = batch_sample[5].to(device)\n",
        "            with torch.set_grad_enabled(False):\n",
        "                output = net(image,question) #salidas de la red\n",
        "                _, pred_indice = torch.max(output, 1)  \n",
        "\n",
        "            pred_answer = dataset.ans_vocab.idx2word(pred_indice)\n",
        "            answer = dataset.ans_vocab.idx2word(label)\n",
        "            question_str = \"\"\n",
        "            for x in question[0]:\n",
        "                if x!=0:\n",
        "                    if dataset.qst_vocab.idx2word(x)=='<unk>':\n",
        "                        question_str+='?'\n",
        "                    else:\n",
        "                        question_str+=dataset.qst_vocab.idx2word(x)+' '\n",
        "                \n",
        "            question_str\n",
        "            print(\"\\n\")\n",
        "            # Visualizar\n",
        "            img = np.squeeze(image.cpu().numpy()).transpose((1, 2, 0))\n",
        "            plt.imshow(img)\n",
        "            plt.suptitle(f\"{question_str}\")\n",
        "            plt.title(f\"Real:{answer} - Pred:{pred_answer}\")\n",
        "            plt.axis('off')\n",
        "            plt.show()\n",
        "\n",
        "            time.sleep(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Se guarda el modelo"
      ],
      "metadata": {
        "id": "P7u-jbNHJBUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.save(model.state_dict(), \"modelo.pth\")"
      ],
      "metadata": {
        "id": "sZxigSIYCM3S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}