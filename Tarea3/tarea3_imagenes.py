# -*- coding: utf-8 -*-
"""Tarea3_imagenes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SGB_DErHvyt9tzlXlOkHAYZHcsNMkLaI
"""

!ls

!unzip /content/imagenes_tarea3_2022.zip

!pwd

#    Para utilizar la interpolacion normal: Interpolacion = 0 
#    Para utilizar la interpolacion bilinear: Interpolacion = 1

interpolacion = 1
gridsearch = True
binario = True

import cv2
import numpy as np

def transform(img):
    #transforma la imagen a escala de grises, al tipo np.float32
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    #redimensiona la imagen a tamaño 64x128
    gray32 = np.float32(gray)
    out = cv2.resize(gray32, (64,128), interpolation = cv2.INTER_AREA)
    return out

example = cv2.imread('pedestrian/1.png')
out = transform(example)
print("shape",out.shape)
out

"""2. Implementar en python una función que reciba una imagen y calcule sus gradientes (se
puede reutilizar código de cálculo de gradientes de la tarea 2).

"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext Cython

# Commented out IPython magic to ensure Python compatibility.
# %%cython
# import cython
# import numpy as np
# cimport numpy as np
# 
# cpdef np.ndarray[np.float32_t, ndim=2] gradx(np.ndarray[np.float32_t, ndim=2] input):
#   # POR HACER: calcular el gradiente en x
#   cdef int rows,cols, i,j
# 
#   cdef np.ndarray[np.float32_t, ndim=2] output=np.zeros([input.shape[0], input.shape[1]], dtype = np.float32)
# 
#   # tamano de la imagen
#   rows = input.shape[0]
#   cols = input.shape[1]
# 
#   for i in range(rows):
#     for j in range(2,cols-2): 
#         output[i][j-2] = -1*input[i][j-2]+0*input[i][j-1]+1*input[i][j]
#         
#   return output

# Commented out IPython magic to ensure Python compatibility.
# %%cython
# import cython
# import numpy as np
# cimport numpy as np
# 
# cpdef np.ndarray[np.float32_t, ndim=2] grady(np.ndarray[np.float32_t, ndim=2] input):
#   # POR HACER: Calcular el gradiente en y
#   cdef int rows,cols, i,j
#   cdef np.ndarray[np.float32_t, ndim=2] output=np.zeros([input.shape[0], input.shape[1]], dtype = np.float32)
# 
#   # tamano de la imagen
#   rows = input.shape[0]
#   cols = input.shape[1]
# 
#   for j in range(cols):
#     for i in range(2,rows-2): 
#         output[i-2][j] = -1*input[i-2][j]+0*input[i-1][j]+1*input[i][j]
#         
#   return output

# Commented out IPython magic to ensure Python compatibility.
# %%cython
# import cython
# import numpy as np
# cimport numpy as np
# 
# cpdef MagDir(np.ndarray[np.float32_t, ndim=2] dx,np.ndarray[np.float32_t, ndim=2] dy):
# 
#     cdef int rows,cols, i,j
#     cdef np.ndarray[np.float32_t, ndim=2] magnitud = np.zeros([dx.shape[0], dx.shape[1]], dtype = np.float32)
#     cdef np.ndarray[np.float32_t, ndim=2] angulo = np.zeros([dx.shape[0], dx.shape[1]], dtype = np.float32)
# 
#     rows = dx.shape[0]
#     cols = dx.shape[1]
#     for i in range(rows):
#         for j in range(cols):
#             magnitud[i][j] = np.sqrt(dx[i][j]**2+dy[i][j]**2)
#             # Angulo en grados para simplificar los futuros histogramas
#             angulo[i][j] = np.degrees(np.arctan2(dx[i][j],dy[i][j]))
# 
#             #los angulos deben quedar en el rango de 0° a 180°
#             if angulo[i][j]>=180:
#                 angulo[i][j] = angulo[i][j] - 180
#     return magnitud,angulo

"""4. Implementar en python el block normalization para el histograma. Para implementar esto,
se deben formar bloques de 2x2 celdas (con traslape). Cada bloque se debe transformar en
un vector de 1x36, el cual se debe normalizar. El vector de características final (de tamaño
1x3780) se obtiene concatenando los vectores normalizados de cada bloque.
"""

def block_normalization(histogram):
    """
    Implementar en python el block normalization para el histograma. Para implementar esto,
    se deben formar bloques de 2x2 celdas (con traslape). Cada bloque se debe transformar en
    un vector de 1x36, el cual se debe normalizar. El vector de características final (de tamaño
    1x3780) se obtiene concatenando los vectores normalizados de cada bloque
    """
    output = np.array([], np.float32)
    for i in range(16-1):
        for j in range(8-1):

            L = np.concatenate((histogram[i][j],histogram[i][j+1],histogram[i+1][j],histogram[i+1][j+1]))
            if sum(L)!=0:
                output= np.concatenate((output,L/sum(L)))
            else:
                output= np.concatenate((output,L))
    
    return np.array(output,np.float32)



def HOG(dx,dy,interpolacion=0):
    """3. Implementar en python una función que, a partir de los gradientes, calcule las
    características HOG usando 8x16 celdas (la salida debe ser un arreglo de numpy de
    dimensión 8x16x9).
    
    Para esto:
    1. se determinan 2 matrices, una de la magnitud de las derivadas y otra
    de las direcciones de estas.
    2. Se dividen las matrices en 8x16 celdas.
    3. En cada celda se calcula un histograma de orientación del gradiente
    con 9 componentes, es decir, se realiza un histograma el cual separa los angulos
    en 9 intervalos cubriendo cada uno 20°, luego por cada pixel de la celda se agregan 
    los valores de la magnitud proporcionales a la distancia del angulo a los intervalos
    más cercanos, de esta manera los votos que se agregan son proporcionales a las distancias.
    4. Luego se reunen los histogramas de orientación del gradiente de cada celda
    los cuales corresponden a los features que retorna este método. Esto se reune
    en un arrelgo de numpy de dimensión 8x16x9.

    Para utilizar la interpolacion normal: Interpolacion = 0 
    Para utilizar la interpolacion bilinear: Interpolacion = 1
    """
    histograms = np.zeros([16,8,9],np.float32)

    #utilizamos una función auxiliar para determinar la magnitud y el angulo
    magnitud, angulo = MagDir(dx,dy)    
    rows = dx.shape[0]
    cols = dx.shape[1]
    
    for i in range(rows):
        for j in range(cols):
            cont1 = i//8
            cont2 = j//8
            #creamos un arreglo que guarde los valores del respectivo histograma
            bins = np.array([0,20,40,60,80,100,120,140,160])
            bin = int(np.floor(angulo[i][j]/20))
            if bin>8:
                bin=8
            p = angulo[i][j]/20-np.floor(angulo[i][j]/20)
            if interpolacion == 0: #lineal
                
                if angulo[i][j]%20==0:
                    histograms[cont1][cont2][bin] += magnitud[i][j]
                elif bin==8:
                    histograms[cont1][cont2][bin] += (1-p)*magnitud[i][j]
                else:
                    histograms[cont1][cont2][bin] += (1-p)*magnitud[i][j]
                    histograms[cont1][cont2][bin+1] += (p)*magnitud[i][j]
            
            else: #bilinearly
                """
                Determinamos las posiciones de los centros de las 4 celdas más cercanas que conforman un bloque,
                luego a partir de la distancia del pixel con esas 4 celdas se distribuyen 8 votos, los cuales corresponden
                a 2 votos por celda: 1 voto al bin correspondiente y otro voto al siguiente bin.
                """
                
                #listas que guardan las 4 celdas más cercanas y sus respectivos posiciones de sus centros
                celdas = [ (cont1,cont2),(cont1,cont2+1),(cont1+1,cont2),(cont1+1,cont2+1) ]
                centros = [((cont1*8+8)/2,(cont2*8+8)/2) ,((cont1*8+8)/2,((cont2+2)*8+8)/2), (((cont1+2)*8+8)/2,(cont2*8+8)/2),(((cont1+2)*8+8)/2,((cont2+2)*8+8)/2)]
                
                #Zona normal
                if celdas[0][0]!=15 and celdas[0][1]!=7:

                    # si estamos en el centro, todos los votos van para esa celda.
                    if i==centros[0][0] and j==centros[0][1]:
                        if angulo[i][j]%20==0:
                            histograms[cont1][cont2][bin] += magnitud[i][j]
                        elif bin==8:
                            histograms[cont1][cont2][bin] += (1-p)*magnitud[i][j]
                        else:
                            histograms[cont1][cont2][bin] += (1-p)*magnitud[i][j]
                            histograms[cont1][cont2][bin+1] += (p)*magnitud[i][j]
                    

                    distancias = []
                    for k in range(4):
                        #determinamos la distancia al centro de la celda px,py
                        px=centros[k][0]
                        py=centros[k][1]

                        d = np.sqrt((i-px)**2+(j-py)**2)
                        distancias.append(d)

                    pd = np.array(distancias,np.float32)/sum(distancias)

                    for k in range(4):
                        c1 = celdas[k][0]
                        c2 = celdas[k][1]
                        #print(pd[3-k],celdas[k])     
                        if angulo[i][j]%20==0:
                            histograms[c1][c2][bin] += magnitud[i][j]*pd[3-k]*0.25
                        elif bin==8:
                            histograms[c1][c2][bin] += (1-p)*magnitud[i][j]*pd[3-k]*0.25
                        else:
                            histograms[c1][c2][bin] += (1-p)*magnitud[i][j]*pd[3-k]*0.25
                            histograms[c1][c2][bin+1] += (p)*magnitud[i][j]*pd[3-k]*0.25

                #Zonas bordes
                else:
                    if angulo[i][j]%20==0:
                        histograms[cont1][cont2][bin] += magnitud[i][j]
                    elif bin==8:
                        histograms[cont1][cont2][bin] += (1-p)*magnitud[i][j]
                    else:
                        histograms[cont1][cont2][bin] += (1-p)*magnitud[i][j]
                        histograms[cont1][cont2][bin+1] += (p)*magnitud[i][j]

            

    features = block_normalization(histograms)

    return features

#Pruebas
i=125
j=62
cont1 =i//8
cont2 = j//8
print("celda:",cont1,cont2)
celdas = [ (cont1,cont2),(cont1,cont2+1),(cont1+1,cont2),(cont1+1,cont2+1) ]
centros = [((cont1*8+8)/2,(cont2*8+8)/2) ,((cont1*8+8)/2,((cont2+2)*8+8)/2), (((cont1+2)*8+8)/2,(cont2*8+8)/2),(((cont1+2)*8+8)/2,((cont2+2)*8+8)/2)]

distancias = []
for k in range(4):
    #determinamos la distancia al centro de la celda px,py
    px=centros[k][0]
    py=centros[k][1]
    

    d = np.sqrt((i-px)**2+(j-py)**2)
    distancias.append(d)

p = np.array(distancias,np.float32)/sum(distancias)


for k in range(4):
    print(p[3-k],celdas[k][0],celdas[k][1])

dx = gradx(out)
print(dx.shape)
dy = grady(out)
result = HOG(dx,dy)
result

result.shape

from google.colab.patches import cv2_imshow
cv2_imshow(example)
cv2_imshow(out)
dx = gradx(out)
dy = grady(out)
magnitud, angulo = MagDir(dx,dy)    
cv2_imshow(magnitud)
cv2_imshow(angulo)

example.shape

out.shape

"""5. Extraer características HOG de cada imagen del conjunto de entrenamiento. Se recomienda
aplicar un StandardScaler (usando scikit-learn) a las características para normalizarlas. El
StandardScaler se debe entrenar usando sólo las características de entrenamiento.

"""

import os

def cargarDatos(nombre_carpeta,clase):
    data = []
    contenido = os.listdir(nombre_carpeta)
    label = []
    for nombre_imagen in contenido: 
        img = cv2.imread(os.path.join(nombre_carpeta, nombre_imagen))
        imgt = transform(img)
        data.append(imgt)
        label.append(clase)

    return np.array(data),np.array(label)

def featureExtractor(data,interpolacion=0):
    """
    Para utilizar la interpolacion normal: Interpolacion = 0 
    Para utilizar la interpolacion bilinear: Interpolacion = 1
    """
    features = []
    for img in data:
        dx = gradx(img)
        dy = grady(img)
        feature = HOG(dx,dy,interpolacion)
        features.append(feature)
    #caracteristicas no normalizadas
    return np.array(features)

#Carga todos los datos de la carpeta "car_side" y le asigna la clase 0
dataCar_side,CSlabel = cargarDatos("car_side",0)
if binario==True:
    label_2 = 0
    text_labels = ["Cars/Chairs","pedestrian"]
else:
    label_2 = 1
    text_labels = ["Cars", "Chairs", "Pedestrian"]

#Carga todos los datos de la carpeta "chair" y le asigna la clase 1
dataChair,CL = cargarDatos("chair",label_2)

#Carga todos los datos de la carpeta "pedestrian" y le asigna la clase 2
dataPedestrian,PL = cargarDatos("pedestrian",2)

print("car_side.   ","shape = ",dataCar_side.shape, ", clase = ","0")
print("chair.      ","shape = ",dataChair.shape,  " , clase = ","1")
print("pedestrian. ","shape = ",dataPedestrian.shape, ", clase = ","2\n")

#data no normalizada
data = np.concatenate((dataCar_side,dataChair,dataPedestrian))
labels = np.concatenate((CSlabel,CL,PL))
data.shape

#Caracteristicas no normalizadas
features = featureExtractor(data,interpolacion=interpolacion)

from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import  PredefinedSplit

# Conjuntos de Train y Val/Test
X_trainval, X_test, y_trainval, y_test = train_test_split(features, labels, test_size=0.2, shuffle = True, stratify = labels)

X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, shuffle = False,test_size=0.25)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)

X_trainval = scaler.transform(X_trainval)
X_test = scaler.transform(X_test)

split_fold = [-1 for _ in range(int(len(X_trainval)*0.75))]+ [0 for _ in range(int(len(X_trainval)*0.25))]
cv = PredefinedSplit(split_fold)

X_train.shape

X_val.shape

X_test.shape

"""6. Entrenar un SVM (usando scikit-learn) con las características extraídas a cada imagen
redimensionada del conjunto de entrenamiento. Se debe elegir un kernel y usar grid
search, usando el conjunto de validación para encontrar los mejores hiper parámetros (se
debe usar PredefinedSplit).

# SVM
"""

from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
# Create the parameter grid based on the results of random search 

param_grid = {'C': [0.1, 1, 10, 100, 1000], 
              'gamma': ['scale','auto',1,0.1],
              'kernel': ['linear','rbf', 'poly', 'sigmoid'],
              'class_weight':('balanced',None),
              'decision_function_shape':['ovo','ovr']} 
# Instantiate the grid search model
grid = GridSearchCV(SVC(), param_grid,cv=cv, refit=False)

if gridsearch == False:
    classifier = SVC(C=0.1, class_weight='balanced', decision_function_shape='ovo',
    gamma= 'scale',kernel='linear')
else:
    # fitting the model for grid search
    grid.fit(X_trainval, y_trainval)
    print(grid.best_params_)
    classifier = SVC(C=0.1, class_weight='balanced', decision_function_shape='ovo',
    gamma= 'scale',kernel='linear')

#classifier = SVC(C=0.1, class_weight='balanced', decision_function_shape='ovo',
#    gamma= 'scale',kernel='linear')
# Interpolacion normal
#classifier = SVC(C=1, class_weight='balanced', decision_function_shape='ovo',
#    gamma= 'scale',kernel='rbf')

classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
print(y_pred)

# calculate accuracy
import matplotlib.pyplot as plt
from sklearn.metrics import  accuracy_score, confusion_matrix, recall_score
from sklearn.metrics import ConfusionMatrixDisplay


accuracy = accuracy_score(y_test, y_pred)*100
recall = recall_score(y_test, y_pred, average='macro')*100
print("Classification accuracy is %2f"%accuracy,"%")
print("Classification recall is %2f"%recall,"%\n")

fig = plt.figure(figsize = (12,9))



ConfusionMatrixDisplay.from_estimator(classifier, X_test, y_test,display_labels=text_labels, normalize="true",cmap=plt.cm.Blues)
t = plt.title("Matriz de confusión SVM")

"""# Random Forest"""

from sklearn.model_selection import GridSearchCV
# Create the parameter grid based on the results of random search 
param_grid = {
    'bootstrap': [False,True],
    'max_depth': [30, 50, 90, 100],
    'max_features': [5,10,15,20,25,30],
    'min_samples_leaf': [3, 4, 5],
    'min_samples_split': [8, 10, 12],
    'n_estimators': [30,60,100,200]
}
rf = RandomForestClassifier()

# Instantiate the grid search model
grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, 
                          cv = 3, verbose = 3)

if gridsearch == False:
    classifierRF = RandomForestClassifier(bootstrap=False, max_depth=90, max_features=30,
                       min_samples_leaf=3, min_samples_split=8)
else:
    # Fit the grid search to the data
    grid_search.fit(X_train, y_train)
    grid_search.best_params_
    #se revisan los mejores hiperparametros encontrados
    classifierRF = grid_search.best_estimator_

#classifierRF = best_grid
#RandomForestClassifier(bootstrap=False, max_depth=100, max_features=25,
#                       min_samples_leaf=4, min_samples_split=10)
#RandomForestClassifier(bootstrap=False, max_depth=90, max_features=30,
#                       min_samples_leaf=3, min_samples_split=8)
classifierRF.fit(X_train, y_train)
y_pred = classifierRF.predict(X_test)
print(y_pred)

# calculate accuracy
import matplotlib.pyplot as plt
from sklearn.metrics import  accuracy_score, confusion_matrix, recall_score
from sklearn.metrics import plot_confusion_matrix

accuracy = accuracy_score(y_test, y_pred)*100
recall = recall_score(y_test, y_pred, average='macro')*100
print("Classification accuracy is %2f"%accuracy,"%")
print("Classification recall is %2f"%recall,"%")

plt.figure(figsize = (12,9))
plot_confusion_matrix(classifierRF, X_test, y_test,display_labels=text_labels,cmap=plt.cm.Blues, normalize="true") 
t = plt.title("Matriz de confusión RF")